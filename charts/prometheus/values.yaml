# Dependencies
kube-prometheus-stack:
  ## Configuration for prometheus-windows-exporter
  ## ref: https://github.com/prometheus-community/helm-charts/tree/main/charts/prometheus-windows-exporter
  ##
  prometheus-windows-exporter:
    ## Enable ServiceMonitor and set Kubernetes label to use as a job label
    ##
    prometheus:
      monitor:
        enabled: false

  ## Provide custom recording or alerting rules to be deployed into the cluster.
  ##
  additionalPrometheusRulesMap:
    rule-name:
      groups:
      - name: instatus_alerts
        rules:
        - alert: ImmichUnhealthy
          expr: argocd_app_info{health_status!="Healthy",name="immich"}
          for: 5m
#          labels:
#            severity: page
#          annotations:
#            summary: High request latency

  ## Configuration for alertmanager
  ## ref: https://prometheus.io/docs/alerting/alertmanager/
  ##
  alertmanager:
    ## Deploy alertmanager
    ##
    enabled: true

    ## Alertmanager configuration directives
    ## ref: https://prometheus.io/docs/alerting/configuration/#configuration-file
    ##      https://prometheus.io/webtools/alerting/routing-tree-editor/
    ##
    config:
      global:
        resolve_timeout: 5m
      inhibit_rules:
        - source_matchers:
            - 'severity = critical'
          target_matchers:
            - 'severity =~ warning|info'
          equal:
            - 'namespace'
            - 'alertname'
        - source_matchers:
            - 'severity = warning'
          target_matchers:
            - 'severity = info'
          equal:
            - 'namespace'
            - 'alertname'
        - source_matchers:
            - 'alertname = InfoInhibitor'
          target_matchers:
            - 'severity = info'
          equal:
            - 'namespace'
      route:
        group_by: ['namespace']
        group_wait: 30s
        group_interval: 5m
        repeat_interval: 12h
        receiver: 'null'
        routes:
        - receiver: 'null'
          matchers:
            - alertname =~ "InfoInhibitor|Watchdog"
        - receiver: 'webhook_receiver_immich'
          matchers:
          - 'alertname = ImmichUnhealthy'
      receivers:
      - name: webhook_receiver_immich
        webhook_configs:
        - url: '<INSERT-YOUR-WEBHOOK>'
          send_resolved: true
      templates:
      - '/etc/alertmanager/config/*.tmpl'

    ## Alertmanager configuration directives (as string type, preferred over the config hash map)
    ## stringConfig will be used only, if tplConfig is true
    ## ref: https://prometheus.io/docs/alerting/configuration/#configuration-file
    ##      https://prometheus.io/webtools/alerting/routing-tree-editor/
    ##
    stringConfig: ""

    ## Pass the Alertmanager configuration directives through Helm's templating
    ## engine. If the Alertmanager configuration contains Alertmanager templates,
    ## they'll need to be properly escaped so that they are not interpreted by
    ## Helm
    ## ref: https://helm.sh/docs/developing_charts/#using-the-tpl-function
    ##      https://prometheus.io/docs/alerting/configuration/#tmpl_string
    ##      https://prometheus.io/docs/alerting/notifications/
    ##      https://prometheus.io/docs/alerting/notification_examples/
    tplConfig: false

    ## Alertmanager template files to format alerts
    ## By default, templateFiles are placed in /etc/alertmanager/config/ and if
    ## they have a .tmpl file suffix will be loaded. See config.templates above
    ## to change, add other suffixes. If adding other suffixes, be sure to update
    ## config.templates above to include those suffixes.
    ## ref: https://prometheus.io/docs/alerting/notifications/
    ##      https://prometheus.io/docs/alerting/notification_examples/
    ##
    templateFiles: {}
    #
    ## An example template:
    #   template_1.tmpl: |-
    #       {{ define "cluster" }}{{ .ExternalURL | reReplaceAll ".*alertmanager\\.(.*)" "$1" }}{{ end }}
    #
    #       {{ define "slack.myorg.text" }}
    #       {{- $root := . -}}
    #       {{ range .Alerts }}
    #         *Alert:* {{ .Annotations.summary }} - `{{ .Labels.severity }}`
    #         *Cluster:* {{ template "cluster" $root }}
    #         *Description:* {{ .Annotations.description }}
    #         *Graph:* <{{ .GeneratorURL }}|:chart_with_upwards_trend:>
    #         *Runbook:* <{{ .Annotations.runbook }}|:spiral_note_pad:>
    #         *Details:*
    #           {{ range .Labels.SortedPairs }} - *{{ .Name }}:* `{{ .Value }}`
    #           {{ end }}
    #       {{ end }}
    #       {{ end }}

    ingress:
      enabled: true
      annotations:
        kubernetes.io/ingress.class: traefik
        kubernetes.io/tls-acme: "true"
      labels: {}
      hosts:
        - alerts.cluster.diluz.io
      ## Paths to use for ingress rules - one path should match the prometheusSpec.routePrefix
      ##
      paths:
        - /

    ## Configuration for Alertmanager secret
    ##
    secret:
      annotations: {}

    ## Settings affecting alertmanagerSpec
    ## ref: https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/api.md#alertmanagerspec
    ##
    alertmanagerSpec:
      ## Storage is the definition of how storage will be used by the Alertmanager instances.
      ## ref: https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/user-guides/storage.md
      ##
      storage:
        volumeClaimTemplate:
          spec:
            storageClassName: nfs-client
            accessModes: ["ReadWriteOnce"]
            resources:
              requests:
                storage: 5Gi


      ## The external URL the Alertmanager instances will be available under. This is necessary to generate correct URLs. This is necessary if Alertmanager is not served from root of a DNS name. string  false
      ##
      externalUrl: 'https://alerts.cluster.diluz.io'

      ## If set to true all actions on the underlying managed objects are not going to be performed, except for delete actions.
      ##
      paused: false

      ## Define resources requests and limits for single Pods.
      ## ref: https://kubernetes.io/docs/user-guide/compute-resources/
      ##
      resources: {}
      # requests:
      #   memory: 400Mi

  ## Using default values from https://github.com/grafana/helm-charts/blob/main/charts/grafana/values.yaml
  ##
  grafana:
    enabled: true
    ingress:
      ## If true, Grafana Ingress will be created
      ##
      enabled: true
      annotations:
        kubernetes.io/ingress.class: traefik
        kubernetes.io/tls-acme: "true"
      labels: {}
      hosts:
        - grafana.cluster.diluz.io
      ## Path for grafana ingress
      path: /
    persistence:
      enabled: true
      storageClassName: nfs-client
      accessModes:
        - ReadWriteOnce
      size: 10Gi

  ## Component scraping the kube api server
  ##
  kubeApiServer:
    enabled: true

  ## Component scraping the kubelet and kubelet-hosted cAdvisor
  ##
  kubelet:
    enabled: false

  ## Component scraping the kube controller manager
  ##
  kubeControllerManager:
    enabled: true

  ## Component scraping etcd
  ##
  kubeEtcd:
    enabled: false

  ## Component scraping kube scheduler
  ##
  kubeScheduler:
    enabled: false

  ## Component scraping kube proxy
  ##
  kubeProxy:
    enabled: false

  ## Component scraping kube state metrics
  ##
  kubeStateMetrics:
    enabled: true

  ## Deploy node exporter as a daemonset to all nodes
  ##
  nodeExporter:
    enabled: true

  ## Deploy a Prometheus instance
  ##
  prometheus:
    enabled: true
    ## Configuration for Prometheus service
    ##
    service:
      ## Service type
      ##
      type: ClusterIP
    ingress:
      enabled: true

      # For Kubernetes >= 1.18 you should specify the ingress-controller via the field ingressClassName
      # See https://kubernetes.io/blog/2020/04/02/improvements-to-the-ingress-api-in-kubernetes-1.18/#specifying-the-class-of-an-ingress
      # ingressClassName: nginx

      annotations:
        kubernetes.io/ingress.class: traefik
        kubernetes.io/tls-acme: "true"
      labels: {}

      ## Redirect ingress to an additional defined port on the service
      # servicePort: 8081

      ## Hostnames.
      ## Must be provided if Ingress is enabled.
      ##
      # hosts:
      #   - prometheus.domain.com
      hosts:
        - prometheus.cluster.diluz.io

      ## Paths to use for ingress rules - one path should match the prometheusSpec.routePrefix
      ##
      paths:
        - /

      ## For Kubernetes >= 1.18 you should specify the pathType (determines how Ingress paths should be matched)
      ## See https://kubernetes.io/blog/2020/04/02/improvements-to-the-ingress-api-in-kubernetes-1.18/#better-path-matching-with-path-types
      # pathType: ImplementationSpecific

      ## TLS configuration for Prometheus Ingress
      ## Secret must be manually created in the namespace
      ##
      tls: []
        # - secretName: prometheus-general-tls
        #   hosts:
        #     - prometheus.example.com
    ## Settings affecting prometheusSpec
    ## ref: https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/api.md#prometheusspec
    ##
    prometheusSpec:
      ## Resource limits & requests
      ##
      resources: {}
      # requests:
      #   memory: 400Mi

      ## Prometheus StorageSpec for persistent data
      ## ref: https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/user-guides/storage.md
      ##
      storageSpec:
        ## Using PersistentVolumeClaim
        ##
        volumeClaimTemplate:
          spec:
            storageClassName: nfs-client
            accessModes: ["ReadWriteOnce"]
            resources:
              requests:
                storage: 50Gi
      #    selector: {}

    additionalServiceMonitors:
      ## Name of the ServiceMonitor to create
      ##
      - name: "argocd-metrics-service-monitor"

        ## Additional labels to set used for the ServiceMonitorSelector. Together with standard labels from
        ## the chart
        ##
        # additionalLabels: {}

        ## Service label for use in assembling a job name of the form <label value>-<port>
        ## If no label is specified, the service name is used.
        ##
        # jobLabel: ""

        ## labels to transfer from the kubernetes service to the target
        ##
        # targetLabels: []

        ## labels to transfer from the kubernetes pods to the target
        ##
        # podTargetLabels: []

        ## Label selector for services to which this ServiceMonitor applies
        ##
        selector:
          matchLabels:
            app.kubernetes.io/name: argocd-metrics
        ## Namespaces from which services are selected
        ##
        namespaceSelector:
          ## Match any namespace
          ##
          # any: false
          ## Explicit list of namespace names to select
          ##
          matchNames:
            - argocd

        ## Endpoints of the selected service to be monitored
        ##
        endpoints:
          ## Name of the endpoint's service port
          ## Mutually exclusive with targetPort
          - port: "metrics"

          ## Name or number of the endpoint's target port
          ## Mutually exclusive with port
          # - targetPort: ""

          ## File containing bearer token to be used when scraping targets
          ##
          #   bearerTokenFile: ""

          ## Interval at which metrics should be scraped
          ##
          #   interval: 30s

          ## HTTP path to scrape for metrics
          ##
            path: /metrics

          ## HTTP scheme to use for scraping
          ##
          #   scheme: http

          ## TLS configuration to use when scraping the endpoint
          ##
          #   tlsConfig:

              ## Path to the CA file
              ##
              # caFile: ""

              ## Path to client certificate file
              ##
              # certFile: ""

              ## Skip certificate verification
              ##
              # insecureSkipVerify: false

              ## Path to client key file
              ##
              # keyFile: ""

              ## Server name used to verify host name
              ##
              # serverName: ""

        ## MetricRelabelConfigs to apply to samples after scraping, but before ingestion.
        ## ref: https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/api.md#relabelconfig
        ##
        # metricRelabelings: []
        # - action: keep
        #   regex: 'kube_(daemonset|deployment|pod|namespace|node|statefulset).+'
        #   sourceLabels: [__name__]

        ## RelabelConfigs to apply to samples before scraping
        ## ref: https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/api.md#relabelconfig
        ##
        # relabelings: []
        # - sourceLabels: [__meta_kubernetes_pod_node_name]
        #   separator: ;
        #   regex: ^(.*)$
        #   targetLabel: nodename
        #   replacement: $1
        #   action: replace
