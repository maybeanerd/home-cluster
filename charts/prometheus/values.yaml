# Dependencies
kube-prometheus-stack:
  ## Configuration for prometheus-windows-exporter
  ## ref: https://github.com/prometheus-community/helm-charts/tree/main/charts/prometheus-windows-exporter
  ##
  prometheus-windows-exporter:
    ## Enable ServiceMonitor and set Kubernetes label to use as a job label
    ##
    prometheus:
      monitor:
        enabled: false

  ## Configuration for alertmanager
  ## ref: https://prometheus.io/docs/alerting/alertmanager/
  ##
  alertmanager:
    ## Deploy alertmanager
    ##
    enabled: true

    ## Annotations for Alertmanager
    ##
    annotations: {}

    ## Api that prometheus will use to communicate with alertmanager. Possible values are v1, v2
    ##
    apiVersion: v2

    ## Service account for Alertmanager to use.
    ## ref: https://kubernetes.io/docs/tasks/configure-pod-container/configure-service-account/
    ##
    serviceAccount:
      create: true
      name: ""
      annotations: {}
      automountServiceAccountToken: true

    ## Configure pod disruption budgets for Alertmanager
    ## ref: https://kubernetes.io/docs/tasks/run-application/configure-pdb/#specifying-a-poddisruptionbudget
    ## This configuration is immutable once created and will require the PDB to be deleted to be changed
    ## https://github.com/kubernetes/kubernetes/issues/45398
    ##
    podDisruptionBudget:
      enabled: false
      minAvailable: 1
      maxUnavailable: ""

    ## Alertmanager configuration directives
    ## ref: https://prometheus.io/docs/alerting/configuration/#configuration-file
    ##      https://prometheus.io/webtools/alerting/routing-tree-editor/
    ##
    config:
      global:
        resolve_timeout: 5m
      inhibit_rules:
        - source_matchers:
            - 'severity = critical'
          target_matchers:
            - 'severity =~ warning|info'
          equal:
            - 'namespace'
            - 'alertname'
        - source_matchers:
            - 'severity = warning'
          target_matchers:
            - 'severity = info'
          equal:
            - 'namespace'
            - 'alertname'
        - source_matchers:
            - 'alertname = InfoInhibitor'
          target_matchers:
            - 'severity = info'
          equal:
            - 'namespace'
      route:
        group_by: ['namespace']
        group_wait: 30s
        group_interval: 5m
        repeat_interval: 12h
        receiver: 'null'
        routes:
        - receiver: 'null'
          matchers:
            - alertname =~ "InfoInhibitor|Watchdog"
      receivers:
      - name: 'null'
      templates:
      - '/etc/alertmanager/config/*.tmpl'

    ## Alertmanager configuration directives (as string type, preferred over the config hash map)
    ## stringConfig will be used only, if tplConfig is true
    ## ref: https://prometheus.io/docs/alerting/configuration/#configuration-file
    ##      https://prometheus.io/webtools/alerting/routing-tree-editor/
    ##
    stringConfig: ""

    ## Pass the Alertmanager configuration directives through Helm's templating
    ## engine. If the Alertmanager configuration contains Alertmanager templates,
    ## they'll need to be properly escaped so that they are not interpreted by
    ## Helm
    ## ref: https://helm.sh/docs/developing_charts/#using-the-tpl-function
    ##      https://prometheus.io/docs/alerting/configuration/#tmpl_string
    ##      https://prometheus.io/docs/alerting/notifications/
    ##      https://prometheus.io/docs/alerting/notification_examples/
    tplConfig: false

    ## Alertmanager template files to format alerts
    ## By default, templateFiles are placed in /etc/alertmanager/config/ and if
    ## they have a .tmpl file suffix will be loaded. See config.templates above
    ## to change, add other suffixes. If adding other suffixes, be sure to update
    ## config.templates above to include those suffixes.
    ## ref: https://prometheus.io/docs/alerting/notifications/
    ##      https://prometheus.io/docs/alerting/notification_examples/
    ##
    templateFiles: {}
    #
    ## An example template:
    #   template_1.tmpl: |-
    #       {{ define "cluster" }}{{ .ExternalURL | reReplaceAll ".*alertmanager\\.(.*)" "$1" }}{{ end }}
    #
    #       {{ define "slack.myorg.text" }}
    #       {{- $root := . -}}
    #       {{ range .Alerts }}
    #         *Alert:* {{ .Annotations.summary }} - `{{ .Labels.severity }}`
    #         *Cluster:* {{ template "cluster" $root }}
    #         *Description:* {{ .Annotations.description }}
    #         *Graph:* <{{ .GeneratorURL }}|:chart_with_upwards_trend:>
    #         *Runbook:* <{{ .Annotations.runbook }}|:spiral_note_pad:>
    #         *Details:*
    #           {{ range .Labels.SortedPairs }} - *{{ .Name }}:* `{{ .Value }}`
    #           {{ end }}
    #       {{ end }}
    #       {{ end }}

    ingress:
      enabled: false

      # For Kubernetes >= 1.18 you should specify the ingress-controller via the field ingressClassName
      # See https://kubernetes.io/blog/2020/04/02/improvements-to-the-ingress-api-in-kubernetes-1.18/#specifying-the-class-of-an-ingress
      # ingressClassName: nginx

      annotations: {}

      labels: {}

      ## Override ingress to a different defined port on the service
      # servicePort: 8081
      ## Override ingress to a different service then the default, this is useful if you need to
      ## point to a specific instance of the alertmanager (eg kube-prometheus-stack-alertmanager-0)
      # serviceName: kube-prometheus-stack-alertmanager-0

      ## Hosts must be provided if Ingress is enabled.
      ##
      hosts: []
        # - alertmanager.domain.com

      ## Paths to use for ingress rules - one path should match the alertmanagerSpec.routePrefix
      ##
      paths: []
      # - /

      ## For Kubernetes >= 1.18 you should specify the pathType (determines how Ingress paths should be matched)
      ## See https://kubernetes.io/blog/2020/04/02/improvements-to-the-ingress-api-in-kubernetes-1.18/#better-path-matching-with-path-types
      # pathType: ImplementationSpecific

      ## TLS configuration for Alertmanager Ingress
      ## Secret must be manually created in the namespace
      ##
      tls: []
      # - secretName: alertmanager-general-tls
      #   hosts:
      #   - alertmanager.example.com

    ## Configuration for Alertmanager secret
    ##
    secret:
      annotations: {}

    ## Configuration for creating an Ingress that will map to each Alertmanager replica service
    ## alertmanager.servicePerReplica must be enabled
    ##
    ingressPerReplica:
      enabled: false

      # For Kubernetes >= 1.18 you should specify the ingress-controller via the field ingressClassName
      # See https://kubernetes.io/blog/2020/04/02/improvements-to-the-ingress-api-in-kubernetes-1.18/#specifying-the-class-of-an-ingress
      # ingressClassName: nginx

      annotations: {}
      labels: {}

      ## Final form of the hostname for each per replica ingress is
      ## {{ ingressPerReplica.hostPrefix }}-{{ $replicaNumber }}.{{ ingressPerReplica.hostDomain }}
      ##
      ## Prefix for the per replica ingress that will have `-$replicaNumber`
      ## appended to the end
      hostPrefix: ""
      ## Domain that will be used for the per replica ingress
      hostDomain: ""

      ## Paths to use for ingress rules
      ##
      paths: []
      # - /

      ## For Kubernetes >= 1.18 you should specify the pathType (determines how Ingress paths should be matched)
      ## See https://kubernetes.io/blog/2020/04/02/improvements-to-the-ingress-api-in-kubernetes-1.18/#better-path-matching-with-path-types
      # pathType: ImplementationSpecific

      ## Secret name containing the TLS certificate for alertmanager per replica ingress
      ## Secret must be manually created in the namespace
      tlsSecretName: ""

      ## Separated secret for each per replica Ingress. Can be used together with cert-manager
      ##
      tlsSecretPerReplica:
        enabled: false
        ## Final form of the secret for each per replica ingress is
        ## {{ tlsSecretPerReplica.prefix }}-{{ $replicaNumber }}
        ##
        prefix: "alertmanager"

    ## Configuration for Alertmanager service
    ##
    service:
      annotations: {}
      labels: {}
      clusterIP: ""

      ## Port for Alertmanager Service to listen on
      ##
      port: 9093
      ## To be used with a proxy extraContainer port
      ##
      targetPort: 9093
      ## Port to expose on each node
      ## Only used if service.type is 'NodePort'
      ##
      nodePort: 30903
      ## List of IP addresses at which the Prometheus server service is available
      ## Ref: https://kubernetes.io/docs/user-guide/services/#external-ips
      ##

      ## Additional ports to open for Alertmanager service
      ##
      additionalPorts: []
      # - name: oauth-proxy
      #   port: 8081
      #   targetPort: 8081
      # - name: oauth-metrics
      #   port: 8082
      #   targetPort: 8082

      externalIPs: []
      loadBalancerIP: ""
      loadBalancerSourceRanges: []

      ## Denotes if this Service desires to route external traffic to node-local or cluster-wide endpoints
      ##
      externalTrafficPolicy: Cluster

      ## If you want to make sure that connections from a particular client are passed to the same Pod each time
      ## Accepts 'ClientIP' or ''
      ##
      sessionAffinity: ""

      ## Service type
      ##
      type: ClusterIP

    ## Configuration for creating a separate Service for each statefulset Alertmanager replica
    ##
    servicePerReplica:
      enabled: false
      annotations: {}

      ## Port for Alertmanager Service per replica to listen on
      ##
      port: 9093

      ## To be used with a proxy extraContainer port
      targetPort: 9093

      ## Port to expose on each node
      ## Only used if servicePerReplica.type is 'NodePort'
      ##
      nodePort: 30904

      ## Loadbalancer source IP ranges
      ## Only used if servicePerReplica.type is "LoadBalancer"
      loadBalancerSourceRanges: []

      ## Denotes if this Service desires to route external traffic to node-local or cluster-wide endpoints
      ##
      externalTrafficPolicy: Cluster

      ## Service type
      ##
      type: ClusterIP

    ## If true, create a serviceMonitor for alertmanager
    ##
    serviceMonitor:
      ## Scrape interval. If not set, the Prometheus default scrape interval is used.
      ##
      interval: ""
      selfMonitor: true

      ## Additional labels
      ##
      additionalLabels: {}

      ## SampleLimit defines per-scrape limit on number of scraped samples that will be accepted.
      ##
      sampleLimit: 0

      ## TargetLimit defines a limit on the number of scraped targets that will be accepted.
      ##
      targetLimit: 0

      ## Per-scrape limit on number of labels that will be accepted for a sample. Only valid in Prometheus versions 2.27.0 and newer.
      ##
      labelLimit: 0

      ## Per-scrape limit on length of labels name that will be accepted for a sample. Only valid in Prometheus versions 2.27.0 and newer.
      ##
      labelNameLengthLimit: 0

      ## Per-scrape limit on length of labels value that will be accepted for a sample. Only valid in Prometheus versions 2.27.0 and newer.
      ##
      labelValueLengthLimit: 0

      ## proxyUrl: URL of a proxy that should be used for scraping.
      ##
      proxyUrl: ""

      ## scheme: HTTP scheme to use for scraping. Can be used with `tlsConfig` for example if using istio mTLS.
      scheme: ""

      ## enableHttp2: Whether to enable HTTP2.
      ## See https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/api.md#endpoint
      enableHttp2: true

      ## tlsConfig: TLS configuration to use when scraping the endpoint. For example if using istio mTLS.
      ## Of type: https://github.com/coreos/prometheus-operator/blob/main/Documentation/api.md#tlsconfig
      tlsConfig: {}

      bearerTokenFile:

      ## MetricRelabelConfigs to apply to samples after scraping, but before ingestion.
      ## ref: https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/api.md#relabelconfig
      ##
      metricRelabelings: []
      # - action: keep
      #   regex: 'kube_(daemonset|deployment|pod|namespace|node|statefulset).+'
      #   sourceLabels: [__name__]

      ## RelabelConfigs to apply to samples before scraping
      ## ref: https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/api.md#relabelconfig
      ##
      relabelings: []
      # - sourceLabels: [__meta_kubernetes_pod_node_name]
      #   separator: ;
      #   regex: ^(.*)$
      #   targetLabel: nodename
      #   replacement: $1
      #   action: replace

      ## Additional Endpoints
      ##
      additionalEndpoints: []
      # - port: oauth-metrics
      #   path: /metrics

    ## Settings affecting alertmanagerSpec
    ## ref: https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/api.md#alertmanagerspec
    ##
    alertmanagerSpec:
      ## Standard object's metadata. More info: https://github.com/kubernetes/community/blob/master/contributors/devel/sig-architecture/api-conventions.md#metadata
      ## Metadata Labels and Annotations gets propagated to the Alertmanager pods.
      ##
      podMetadata: {}

      ## Image of Alertmanager
      ##
      image:
        registry: quay.io
        repository: prometheus/alertmanager
        tag: v0.26.0
        sha: ""

      ## If true then the user will be responsible to provide a secret with alertmanager configuration
      ## So when true the config part will be ignored (including templateFiles) and the one in the secret will be used
      ##
      useExistingSecret: false

      ## Secrets is a list of Secrets in the same namespace as the Alertmanager object, which shall be mounted into the
      ## Alertmanager Pods. The Secrets are mounted into /etc/alertmanager/secrets/.
      ##
      secrets: []

      ## ConfigMaps is a list of ConfigMaps in the same namespace as the Alertmanager object, which shall be mounted into the Alertmanager Pods.
      ## The ConfigMaps are mounted into /etc/alertmanager/configmaps/.
      ##
      configMaps: []

      ## ConfigSecret is the name of a Kubernetes Secret in the same namespace as the Alertmanager object, which contains configuration for
      ## this Alertmanager instance. Defaults to 'alertmanager-' The secret is mounted into /etc/alertmanager/config.
      ##
      # configSecret:

      ## WebTLSConfig defines the TLS parameters for HTTPS
      ## ref: https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/api.md#alertmanagerwebspec
      web: {}

      ## AlertmanagerConfigs to be selected to merge and configure Alertmanager with.
      ##
      alertmanagerConfigSelector: {}
      ## Example which selects all alertmanagerConfig resources
      ## with label "alertconfig" with values any of "example-config" or "example-config-2"
      # alertmanagerConfigSelector:
      #   matchExpressions:
      #     - key: alertconfig
      #       operator: In
      #       values:
      #         - example-config
      #         - example-config-2
      #
      ## Example which selects all alertmanagerConfig resources with label "role" set to "example-config"
      # alertmanagerConfigSelector:
      #   matchLabels:
      #     role: example-config

      ## Namespaces to be selected for AlertmanagerConfig discovery. If nil, only check own namespace.
      ##
      alertmanagerConfigNamespaceSelector: {}
      ## Example which selects all namespaces
      ## with label "alertmanagerconfig" with values any of "example-namespace" or "example-namespace-2"
      # alertmanagerConfigNamespaceSelector:
      #   matchExpressions:
      #     - key: alertmanagerconfig
      #       operator: In
      #       values:
      #         - example-namespace
      #         - example-namespace-2

      ## Example which selects all namespaces with label "alertmanagerconfig" set to "enabled"
      # alertmanagerConfigNamespaceSelector:
      #   matchLabels:
      #     alertmanagerconfig: enabled

      ## AlermanagerConfig to be used as top level configuration
      ##
      alertmanagerConfiguration: {}
      ## Example with select a global alertmanagerconfig
      # alertmanagerConfiguration:
      #   name: global-alertmanager-Configuration

      ## Defines the strategy used by AlertmanagerConfig objects to match alerts. eg:
      ##
      alertmanagerConfigMatcherStrategy: {}
      ## Example with use OnNamespace strategy
      # alertmanagerConfigMatcherStrategy:
      #   type: OnNamespace

      ## Define Log Format
      # Use logfmt (default) or json logging
      logFormat: logfmt

      ## Log level for Alertmanager to be configured with.
      ##
      logLevel: info

      ## Size is the expected size of the alertmanager cluster. The controller will eventually make the size of the
      ## running cluster equal to the expected size.
      replicas: 1

      ## Time duration Alertmanager shall retain data for. Default is '120h', and must match the regular expression
      ## [0-9]+(ms|s|m|h) (milliseconds seconds minutes hours).
      ##
      retention: 120h

      ## Storage is the definition of how storage will be used by the Alertmanager instances.
      ## ref: https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/user-guides/storage.md
      ##
      storage: {}
      # volumeClaimTemplate:
      #   spec:
      #     storageClassName: gluster
      #     accessModes: ["ReadWriteOnce"]
      #     resources:
      #       requests:
      #         storage: 50Gi
      #     selector: {}


      ## The external URL the Alertmanager instances will be available under. This is necessary to generate correct URLs. This is necessary if Alertmanager is not served from root of a DNS name. string  false
      ##
      externalUrl:

      ## The route prefix Alertmanager registers HTTP handlers for. This is useful, if using ExternalURL and a proxy is rewriting HTTP routes of a request, and the actual ExternalURL is still true,
      ## but the server serves requests under a different route prefix. For example for use with kubectl proxy.
      ##
      routePrefix: /

      ## scheme: HTTP scheme to use. Can be used with `tlsConfig` for example if using istio mTLS.
      scheme: ""

      ## tlsConfig: TLS configuration to use when connect to the endpoint. For example if using istio mTLS.
      ## Of type: https://github.com/coreos/prometheus-operator/blob/main/Documentation/api.md#tlsconfig
      tlsConfig: {}

      ## If set to true all actions on the underlying managed objects are not going to be performed, except for delete actions.
      ##
      paused: false

      ## Define which Nodes the Pods are scheduled on.
      ## ref: https://kubernetes.io/docs/user-guide/node-selection/
      ##
      nodeSelector: {}

      ## Define resources requests and limits for single Pods.
      ## ref: https://kubernetes.io/docs/user-guide/compute-resources/
      ##
      resources: {}
      # requests:
      #   memory: 400Mi

      ## Pod anti-affinity can prevent the scheduler from placing Prometheus replicas on the same node.
      ## The default value "soft" means that the scheduler should *prefer* to not schedule two replica pods onto the same node but no guarantee is provided.
      ## The value "hard" means that the scheduler is *required* to not schedule two replica pods onto the same node.
      ## The value "" will disable pod anti-affinity so that no anti-affinity rules will be configured.
      ##
      podAntiAffinity: ""

      ## If anti-affinity is enabled sets the topologyKey to use for anti-affinity.
      ## This can be changed to, for example, failure-domain.beta.kubernetes.io/zone
      ##
      podAntiAffinityTopologyKey: kubernetes.io/hostname

      ## Assign custom affinity rules to the alertmanager instance
      ## ref: https://kubernetes.io/docs/concepts/configuration/assign-pod-node/
      ##
      affinity: {}
      # nodeAffinity:
      #   requiredDuringSchedulingIgnoredDuringExecution:
      #     nodeSelectorTerms:
      #     - matchExpressions:
      #       - key: kubernetes.io/e2e-az-name
      #         operator: In
      #         values:
      #         - e2e-az1
      #         - e2e-az2

      ## If specified, the pod's tolerations.
      ## ref: https://kubernetes.io/docs/concepts/configuration/taint-and-toleration/
      ##
      tolerations: []
      # - key: "key"
      #   operator: "Equal"
      #   value: "value"
      #   effect: "NoSchedule"

      ## If specified, the pod's topology spread constraints.
      ## ref: https://kubernetes.io/docs/concepts/workloads/pods/pod-topology-spread-constraints/
      ##
      topologySpreadConstraints: []
      # - maxSkew: 1
      #   topologyKey: topology.kubernetes.io/zone
      #   whenUnsatisfiable: DoNotSchedule
      #   labelSelector:
      #     matchLabels:
      #       app: alertmanager

      ## SecurityContext holds pod-level security attributes and common container settings.
      ## This defaults to non root user with uid 1000 and gid 2000. *v1.PodSecurityContext  false
      ## ref: https://kubernetes.io/docs/tasks/configure-pod-container/security-context/
      ##
      securityContext:
        runAsGroup: 2000
        runAsNonRoot: true
        runAsUser: 1000
        fsGroup: 2000
        seccompProfile:
          type: RuntimeDefault

      ## ListenLocal makes the Alertmanager server listen on loopback, so that it does not bind against the Pod IP.
      ## Note this is only for the Alertmanager UI, not the gossip communication.
      ##
      listenLocal: false

      ## Containers allows injecting additional containers. This is meant to allow adding an authentication proxy to an Alertmanager pod.
      ##
      containers: []
      # containers:
      # - name: oauth-proxy
      #   image: quay.io/oauth2-proxy/oauth2-proxy:v7.5.1
      #   args:
      #   - --upstream=http://127.0.0.1:9093
      #   - --http-address=0.0.0.0:8081
      #   - --metrics-address=0.0.0.0:8082
      #   - ...
      #   ports:
      #   - containerPort: 8081
      #     name: oauth-proxy
      #     protocol: TCP
      #   - containerPort: 8082
      #     name: oauth-metrics
      #     protocol: TCP
      #   resources: {}

      # Additional volumes on the output StatefulSet definition.
      volumes: []

      # Additional VolumeMounts on the output StatefulSet definition.
      volumeMounts: []

      ## InitContainers allows injecting additional initContainers. This is meant to allow doing some changes
      ## (permissions, dir tree) on mounted volumes before starting prometheus
      initContainers: []

      ## Priority class assigned to the Pods
      ##
      priorityClassName: ""

      ## AdditionalPeers allows injecting a set of additional Alertmanagers to peer with to form a highly available cluster.
      ##
      additionalPeers: []

      ## PortName to use for Alert Manager.
      ##
      portName: "http-web"

      ## ClusterAdvertiseAddress is the explicit address to advertise in cluster. Needs to be provided for non RFC1918 [1] (public) addresses. [1] RFC1918: https://tools.ietf.org/html/rfc1918
      ##
      clusterAdvertiseAddress: false

      ## clusterGossipInterval determines interval between gossip attempts.
      ## Needs to be specified as GoDuration, a time duration that can be parsed by Go’s time.ParseDuration() (e.g. 45ms, 30s, 1m, 1h20m15s)
      clusterGossipInterval: ""

      ## clusterPeerTimeout determines timeout for cluster peering.
      ## Needs to be specified as GoDuration, a time duration that can be parsed by Go’s time.ParseDuration() (e.g. 45ms, 30s, 1m, 1h20m15s)
      clusterPeerTimeout: ""

      ## clusterPushpullInterval determines interval between pushpull attempts.
      ## Needs to be specified as GoDuration, a time duration that can be parsed by Go’s time.ParseDuration() (e.g. 45ms, 30s, 1m, 1h20m15s)
      clusterPushpullInterval: ""

      ## ForceEnableClusterMode ensures Alertmanager does not deactivate the cluster mode when running with a single replica.
      ## Use case is e.g. spanning an Alertmanager cluster across Kubernetes clusters with a single replica in each.
      forceEnableClusterMode: false

      ## Minimum number of seconds for which a newly created pod should be ready without any of its container crashing for it to
      ## be considered available. Defaults to 0 (pod will be considered available as soon as it is ready).
      minReadySeconds: 0

      ## Additional configuration which is not covered by the properties above. (passed through tpl)
      additionalConfig: {}

      ## Additional configuration which is not covered by the properties above.
      ## Useful, if you need advanced templating inside alertmanagerSpec.
      ## Otherwise, use alertmanager.alertmanagerSpec.additionalConfig (passed through tpl)
      additionalConfigString: ""

    ## ExtraSecret can be used to store various data in an extra secret
    ## (use it for example to store hashed basic auth credentials)
    extraSecret:
      ## if not set, name will be auto generated
      # name: ""
      annotations: {}
      data: {}
    #   auth: |
    #     foo:$apr1$OFG3Xybp$ckL0FHDAkoXYIlH9.cysT0
    #     someoneelse:$apr1$DMZX2Z4q$6SbQIfyuLQd.xmo/P0m2c.

  ## Using default values from https://github.com/grafana/helm-charts/blob/main/charts/grafana/values.yaml
  ##
  grafana:
    # TODO maybe we want to use grafana
    enabled: false
    ingress:
      ## If true, Grafana Ingress will be created
      ##
      enabled: false

      ## IngressClassName for Grafana Ingress.
      ## Should be provided if Ingress is enable.
      ##
      # ingressClassName: nginx

      ## Annotations for Grafana Ingress
      ##
      annotations: {}
        # kubernetes.io/ingress.class: nginx
        # kubernetes.io/tls-acme: "true"

      ## Labels to be added to the Ingress
      ##
      labels: {}

      ## Hostnames.
      ## Must be provided if Ingress is enable.
      ##
      # hosts:
      #   - grafana.domain.com
      hosts: []

      ## Path for grafana ingress
      path: /

      ## TLS configuration for grafana Ingress
      ## Secret must be manually created in the namespace
      ##
      tls: []
      # - secretName: grafana-general-tls
      #   hosts:
      #   - grafana.example.com

  ## Component scraping the kube api server
  ##
  kubeApiServer:
    enabled: true

  ## Component scraping the kubelet and kubelet-hosted cAdvisor
  ##
  kubelet:
    enabled: false

  ## Component scraping the kube controller manager
  ##
  kubeControllerManager:
    enabled: true

  ## Component scraping etcd
  ##
  kubeEtcd:
    enabled: false

  ## Component scraping kube scheduler
  ##
  kubeScheduler:
    enabled: false

  ## Component scraping kube proxy
  ##
  kubeProxy:
    enabled: false

  ## Component scraping kube state metrics
  ##
  kubeStateMetrics:
    enabled: true

  ## Deploy node exporter as a daemonset to all nodes
  ##
  nodeExporter:
    enabled: true

  ## Deploy a Prometheus instance
  ##
  prometheus:
    enabled: true
    ## Configuration for Prometheus service
    ##
    service:
      ## Service type
      ##
      type: ClusterIP
    ingress:
      enabled: true

      # For Kubernetes >= 1.18 you should specify the ingress-controller via the field ingressClassName
      # See https://kubernetes.io/blog/2020/04/02/improvements-to-the-ingress-api-in-kubernetes-1.18/#specifying-the-class-of-an-ingress
      # ingressClassName: nginx

      annotations:
        kubernetes.io/ingress.class: traefik
        kubernetes.io/tls-acme: "true"
      labels: {}

      ## Redirect ingress to an additional defined port on the service
      # servicePort: 8081

      ## Hostnames.
      ## Must be provided if Ingress is enabled.
      ##
      # hosts:
      #   - prometheus.domain.com
      hosts:
        - prometheus.cluster.diluz.io

      ## Paths to use for ingress rules - one path should match the prometheusSpec.routePrefix
      ##
      paths:
        - /

      ## For Kubernetes >= 1.18 you should specify the pathType (determines how Ingress paths should be matched)
      ## See https://kubernetes.io/blog/2020/04/02/improvements-to-the-ingress-api-in-kubernetes-1.18/#better-path-matching-with-path-types
      # pathType: ImplementationSpecific

      ## TLS configuration for Prometheus Ingress
      ## Secret must be manually created in the namespace
      ##
      tls: []
        # - secretName: prometheus-general-tls
        #   hosts:
        #     - prometheus.example.com
    ## Settings affecting prometheusSpec
    ## ref: https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/api.md#prometheusspec
    ##
    prometheusSpec:
      ## Resource limits & requests
      ##
      resources: {}
      # requests:
      #   memory: 400Mi

      ## Prometheus StorageSpec for persistent data
      ## ref: https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/user-guides/storage.md
      ##
      storageSpec:
        ## Using PersistentVolumeClaim
        ##
        volumeClaimTemplate:
          spec:
            storageClassName: nfs-client
            accessModes: ["ReadWriteOnce"]
            resources:
              requests:
                storage: 50Gi
      #    selector: {}

    additionalServiceMonitors: 
      ## Name of the ServiceMonitor to create
      ##
      - name: "argocd-metrics-service-monitor"

        ## Additional labels to set used for the ServiceMonitorSelector. Together with standard labels from
        ## the chart
        ##
        # additionalLabels: {}

        ## Service label for use in assembling a job name of the form <label value>-<port>
        ## If no label is specified, the service name is used.
        ##
        # jobLabel: ""

        ## labels to transfer from the kubernetes service to the target
        ##
        # targetLabels: []

        ## labels to transfer from the kubernetes pods to the target
        ##
        # podTargetLabels: []

        ## Label selector for services to which this ServiceMonitor applies
        ##
        selector:
          matchLabels:
            app.kubernetes.io/name: argocd-metrics
        ## Namespaces from which services are selected
        ##
        namespaceSelector:
          ## Match any namespace
          ##
          # any: false
          ## Explicit list of namespace names to select
          ##
          matchNames:
            - argocd

        ## Endpoints of the selected service to be monitored
        ##
        endpoints:
          ## Name of the endpoint's service port
          ## Mutually exclusive with targetPort
          - port: "metrics"

          ## Name or number of the endpoint's target port
          ## Mutually exclusive with port
          # - targetPort: ""

          ## File containing bearer token to be used when scraping targets
          ##
          #   bearerTokenFile: ""

          ## Interval at which metrics should be scraped
          ##
          #   interval: 30s

          ## HTTP path to scrape for metrics
          ##
            path: /metrics

          ## HTTP scheme to use for scraping
          ##
          #   scheme: http

          ## TLS configuration to use when scraping the endpoint
          ##
          #   tlsConfig:

              ## Path to the CA file
              ##
              # caFile: ""

              ## Path to client certificate file
              ##
              # certFile: ""

              ## Skip certificate verification
              ##
              # insecureSkipVerify: false

              ## Path to client key file
              ##
              # keyFile: ""

              ## Server name used to verify host name
              ##
              # serverName: ""

        ## MetricRelabelConfigs to apply to samples after scraping, but before ingestion.
        ## ref: https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/api.md#relabelconfig
        ##
        # metricRelabelings: []
        # - action: keep
        #   regex: 'kube_(daemonset|deployment|pod|namespace|node|statefulset).+'
        #   sourceLabels: [__name__]

        ## RelabelConfigs to apply to samples before scraping
        ## ref: https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/api.md#relabelconfig
        ##
        # relabelings: []
        # - sourceLabels: [__meta_kubernetes_pod_node_name]
        #   separator: ;
        #   regex: ^(.*)$
        #   targetLabel: nodename
        #   replacement: $1
        #   action: replace
